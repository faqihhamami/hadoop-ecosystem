---install java

1. check java #java -version
2. if there isnt, install it #apt-get install default-jdk
3. check location of jdk #update-alternatives --list java

---install hadoop
1. create user 
	#adduser hadoop
	#passwd hadoop
2. make admin #usermod -aG sudo <username>
3. switch the user
4. setup ssh
	$ ssh-keygen -t rsa
	$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
	$ chmod 0600 ~/.ssh/authorized_keys
	$ ssh localhost (if error apt-get install ssh)
	$ exit
5. unpack hadoop and rename it
	$ tar xzf hadoop-2.7.1.tar.gz
	$ mv hadoop-2.7.1 hadoop
6. Setup Environment Variables #gedit ~/.bashrc and append this script
	export HADOOP_HOME=/home/hadoop/hadoop //path where install
	export HADOOP_INSTALL=$HADOOP_HOME
	export HADOOP_MAPRED_HOME=$HADOOP_HOME
	export HADOOP_COMMON_HOME=$HADOOP_HOME
	export HADOOP_HDFS_HOME=$HADOOP_HOME
	export YARN_HOME=$HADOOP_HOME
	export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
	export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
7. apply the bashrc file $ source ~/.bashrc
8. edit hadoop file configuration /home/<user>/hadoop/etc/hadoop
	Edit hadoop/etc/hadoop/core-site.xml:
	<configuration>
	<property>
	<name>fs.defaultFS</name>
	<value>hdfs://localhost:9000</value>
	</property>
	</configuration>
	
	Edit hadoop/etc/hadoop/hdfs-site.xml:
	#mkdir /home/<USER>/pseudo/
	<configuration>
	<property>
	  <name>dfs.name.dir</name>
	  <value>file:///home/<USER>/pseudo/dfs/name</value>
	</property>
	<property>
	  <name>dfs.data.dir</name>
	  <value>file:///home/<USER>/pseudo/dfs/data</value>
	</property>
	<property>
	<name>dfs.replication</name>
	<value>1</value>
	</property>
	</configuration>

	Edit mapred-site.xml
	<configuration>
	<property>
	<name>mapreduce.framework.name</name>
	<value>yarn</value>
	</property>
	</configuration>

	Edit yarn-site.xml
	<configuration>
	<property>
	<name>yarn.nodemanager.aux-services</name>
	<value>mapreduce_shuffle</value>
	</property>
	</configuration>
9. Edit /home/hadoop/hadoop/etc/hadoop/hadoop-env.sh
	export JAVA_HOME=<java path>
10. Format Namenode $ hdfs namenode -format
11. run hadoop $start-all.sh / start-dfs.sh, start-yarn.sh
12. check hadoop running #jps
13. Go to the browser and open http://127.0.0.1:50070
